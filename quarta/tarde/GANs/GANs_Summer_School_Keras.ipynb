{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks in Keras\n",
    "\n",
    "First let's import all the necessary libraries\n",
    "In this case Keras is running in a Tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Input, Dense, Lambda, Merge\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Reshape\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
    "import random\n",
    "from keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, each digit type is adjusted to 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step selects a small sample from MNIST so that you can do the computation even without a GPU\n",
    "After that, we add noise to data, using a random normal distribution with mean equal to zero and standard deviation equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=60\n",
    "\n",
    "x_train=x_train[0:n]\n",
    "x_test=x_test[n:n+n]\n",
    "\n",
    "noise_factor = 0.1\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape inputs for the Neural Network that will be composed of real data and data with noise addded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape((n,28,28,1))\n",
    "x_test=x_test.reshape((n,28,28,1))\n",
    "x_train_noisy = x_train_noisy.reshape((n,28,28,1))\n",
    "x_test_noisy = x_test_noisy.reshape((n,28,28,1))\n",
    "\n",
    "x_train_noisy=np.concatenate([x_train_noisy,x_train]).reshape(2*n,28,28,1)\n",
    "x_train=np.concatenate([x_train,x_train]).reshape(2*n,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a random sample from real data + noisy data and get one-hot encodes for y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)\n",
    "sel=random.sample(range(0,x_train.shape[0]), 100)\n",
    "y_train=pd.get_dummies(y_train[sel])\n",
    "x_train_noisy=x_train_noisy[sel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "nb_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "nb_filters = 32\n",
    "pool_size = (2, 2)\n",
    "kernel_size = (3, 3)\n",
    "input_shape=(28,28,1)\n",
    "learning_rate = 0.008\n",
    "decay_rate = 5e-5\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we develop the Generative part of the GAN using Keras and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 28,353\n",
      "Trainable params: 28,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=learning_rate,momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "generator = Model(input_img, decoded)\n",
    "generator.compile(loss='mean_squared_error', optimizer=sgd,metrics = ['accuracy'])\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the Discriminator part of the GAN, using model.add instead of x=layer(x) to diversify our knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 600,810\n",
      "Trainable params: 600,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='valid',\n",
    "                        input_shape=input_shape))\n",
    "discriminator.add(Activation('relu'))\n",
    "discriminator.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1])))\n",
    "discriminator.add(Activation('relu'))\n",
    "discriminator.add(MaxPooling2D(pool_size=pool_size))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(128))\n",
    "discriminator.add(Activation('relu'))\n",
    "discriminator.add(Dropout(0.5))\n",
    "discriminator.add(Dense(10))\n",
    "discriminator.add(Activation('softmax'))\n",
    "discriminator.compile(loss='categorical_crossentropy', optimizer=sgd,metrics = ['accuracy'])\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we'll create a function that will allow us to freeze the training of the Generator (OR NOT) so that we can try different update strategies with Discriminator and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for k in net.layers:\n",
    "       k.trainable = val\n",
    "trainable(generator, False)\n",
    "\n",
    "gan_input = Input(batch_shape=(None, 28,28,1))\n",
    "\n",
    "gan_level2 = discriminator(generator([gan_input]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN = Model(gan_input, gan_level2)\n",
    "GAN.compile(loss='mean_squared_error', optimizer=sgd,metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we will update Discriminator and Generator asynchronously in the proportion 1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator epoch 0\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.3335 - acc: 0.0900       \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0897 - acc: 0.1600     \n",
      "Discriminator epoch 1\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.2853 - acc: 0.1500     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0896 - acc: 0.1200     \n",
      "Discriminator epoch 2\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.2848 - acc: 0.1600     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0894 - acc: 0.1200     \n",
      "Discriminator epoch 3\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.2484 - acc: 0.1700     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0896 - acc: 0.1400     \n",
      "Discriminator epoch 4\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.2337 - acc: 0.1500     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0890 - acc: 0.1400     \n",
      "Discriminator epoch 5\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.2214 - acc: 0.2100     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0895 - acc: 0.1600     \n",
      "Discriminator epoch 6\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.1978 - acc: 0.2300     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0894 - acc: 0.1500     \n",
      "Discriminator epoch 7\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.1634 - acc: 0.2700     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0895 - acc: 0.1800     \n",
      "Discriminator epoch 8\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.1150 - acc: 0.2900     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0910 - acc: 0.0800     \n",
      "Discriminator epoch 9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 2.1042 - acc: 0.2800     \n",
      "Generator\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s - loss: 0.0909 - acc: 0.1300     \n"
     ]
    }
   ],
   "source": [
    "nb_epochs=1\n",
    "rate=5\n",
    "for i in range(0,10):\n",
    "    print('Discriminator epoch', i)\n",
    "    discriminator.fit(x_train_noisy,np.array(y_train),\n",
    "                    epochs=nb_epochs,\n",
    "                    batch_size=30,verbose=1)\n",
    "    print('Generator')\n",
    "    GAN.fit(x_train_noisy,np.array(y_train),\n",
    "            batch_size=30, epochs=nb_epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reset all variables that were reshaped and one-hot encoded to calculate accuracy in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "y_train=y_train[0:n]\n",
    "y_train=np.concatenate([y_train,y_train])[sel]\n",
    "y_test=y_test[0:n]\n",
    "y_test=np.concatenate([y_test,y_test])[sel]\n",
    "x_test_noisy=x_test_noisy[0:n]\n",
    "x_test_noisy=np.concatenate([x_test_noisy,x_test_noisy])[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 0.11\n",
      "Accuracy Test: 0.09\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Train:',accuracy_score(y_train,np.argmax(GAN.predict(x_train_noisy),axis=1)))\n",
    "\n",
    "print('Accuracy Test:',accuracy_score(y_test,np.argmax(GAN.predict(x_test_noisy),axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
