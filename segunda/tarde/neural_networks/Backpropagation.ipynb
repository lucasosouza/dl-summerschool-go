{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objetivos__: \n",
    "\n",
    "- entender os conceitos de derivada e gradiente\n",
    "- entender a diferença entre gradiente analítico e numérico\n",
    "- aprender a calcular a backpropagação de qualquer rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0. Imports and Configurações](#0.-Imports-and-Configurações)\n",
    "\n",
    "[1. Introdução](#1.-Introdução)\n",
    "- [O Objetivo](#O-Objetivo)\n",
    "- [Estratégia 1: Busca Aleatória](#Estratégia-1:-Busca-Aleatória)\n",
    "- [Estratégia 2: Busca Aleatória Local](#Estratégia-2:-Busca-Aleatória-Local)\n",
    "- [Estratégia 3: Gradiente Numérico](#Estratégia-3:-Gradiente-Numérico)\n",
    "- [Estratégia 4: Gradiente Analítico](#Estratégia-4:-Gradiente-Anal%C3%ADtico)\n",
    "- [Caso Recursivo: Múltiplas Portas](#Caso-Recursivo:-Múltiplas-Portas)\n",
    "- [Checagem do gradiente numérico](#Checagem-do-gradiente-numérico)\n",
    "- [Neurônio Sigmóide](#Neurônio-Sigmóide)\n",
    "\n",
    "[2. Backpropagation](#2.-Backpropagation)\n",
    "- [Se tornando um Ninja em Backpropagation!](#Se-tornando-um-Ninja-em-Backpropagation!)\n",
    "- [Resumo dos Padrões na Backpropagation](#Resumo-dos-Padrões-na-Backpropagation)\n",
    "- [Exemplo 1](#Exemplo-1)\n",
    "- [Exemplo 2](#Exemplo-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:25.803460Z",
     "start_time": "2017-09-12T19:39:18.946627Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A melhor maneira de pensar em redes neurais é como circuitos de valores reais. Mas, ao invés de valores booleanos, valores reais e, ao invés de portas lógicas como **and** ou **or**, portas binárias (dois operandos) como $*$ (multiplicação), + (adição), max, exp, etc. Além disso, também teremos **gradientes** fluindo pelo circuito, mas na direção oposta.\n",
    "\n",
    "<img src='images/porta_multiplicacao.png' width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:25.821474Z",
     "start_time": "2017-09-12T19:39:25.807463Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardMultiplyGate(a, b):\n",
    "    return a*b\n",
    "\n",
    "forwardMultiplyGate(-2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma matemática, a gente pode considerar que essa porta implementa a seguinte função:\n",
    "\n",
    "$$f(x,y)=x*y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos imaginar que temos o seguinte problema:\n",
    "1. Nós vamos providenciar a um circuito valores específicos como entrada (x=-2, y=3)\n",
    "2. O circuito vai calcular o valor de saída (-6)\n",
    "3. A questão é: *Quanto mudar a entrada para levemente **aumentar** a saída?*\n",
    "\n",
    "No nosso caso, em que direção devemos mudar x,y para conseguir um número maior que -6? Note que, pro nosso exemplo, se x = -1.99 e y = 2.99, x$*$y = -5.95 que é maior que -6. **-5.95 é melhor (maior) que 6**, e obtivemos uma melhora de 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estratégia 1: Busca Aleatória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Isso não é trivial? A gente pode simplesmente gerar valores aleatórios, calcular a saída e guardar o melhor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:25.998751Z",
     "start_time": "2017-09-12T19:39:25.827478Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = -2,3\n",
    "melhor_saida = forwardMultiplyGate(x,y)\n",
    "melhor_x, melhor_y = 0,0\n",
    "\n",
    "for k in range(0,100):\n",
    "    x_try = 5*np.random.random() - 5\n",
    "    y_try = 5*np.random.random() - 5\n",
    "    out = forwardMultiplyGate(x_try, y_try)\n",
    "    \n",
    "    if out > melhor_saida:\n",
    "        melhor_saida = out\n",
    "        melhor_x, melhor_y = x_try, y_try\n",
    "\n",
    "print(melhor_x, melhor_y, forwardMultiplyGate(melhor_x, melhor_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, foi bem melhor. Mas, e se tivermos milhões de entradas? É claro que essa estratégia não funcionará. Vamos tentar algo mais aprimorado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estratégia 2: Busca Aleatória Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:26.158036Z",
     "start_time": "2017-09-12T19:39:26.003755Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = -2,3\n",
    "passo = 0.01\n",
    "melhor_saida = forwardMultiplyGate(x,y)\n",
    "melhor_x, melhor_y = 0,0\n",
    "\n",
    "for k in range(0,100):\n",
    "    x_try = x + passo * (2*np.random.random() - 1)\n",
    "    y_try = y + passo * (2*np.random.random() - 1)  \n",
    "    out = forwardMultiplyGate(x_try, y_try)\n",
    "    \n",
    "    if out > melhor_saida:\n",
    "        melhor_saida = out\n",
    "        melhor_x, melhor_y = x_try, y_try\n",
    "\n",
    "print(melhor_x, melhor_y, forwardMultiplyGate(melhor_x, melhor_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Estratégia 3: Gradiente Numérico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine agora que a gente pega as entradas de um circuito e puxa-as para uma direção positiva. Essa força puxando $x$ e $y$ vai nos dizer como $x$ e $y$ devem mudar para aumentar a saída. Não entendeu? Vamos explicar:\n",
    "\n",
    "Se olharmos para as entradas, a gente pode intuitivamente ver que a força em $x$ deveria sempre ser positiva, porque tornando $x$ um pouquinho maior de $x=-2$ para $x=-1$ aumenta a saída do circuito para $-3$, o que é bem maior que $-6$. Por outro lado, se a força em $y$ for negativa, tornando-o menor, como de $y=3$ para $y=2$, também aumenta a saída: $-2\\times2 = -4$, de novo maior que $-6$.\n",
    "\n",
    "E como calcular essa força? Usando **derivadas**.\n",
    "\n",
    "> *A derivada pode ser pensada como a força que a gente aplica em cada entrada para aumentar a saída*\n",
    "\n",
    "<img src='images/derivada.gif'>\n",
    "\n",
    "E como exatamente a gente vai fazer isso? Em vez de olhar para o valor de saída, como fizemos anteriormente, nós vamos iterar sobre as cada entrada individualmente, aumentando-as bem devagar e vendo o que acontece com a saída. **A quantidade que a saída muda é a resposta da derivada**.\n",
    "\n",
    "Vamos para definição matemática. A derivada em relação a x da nossa porta pode ser definida como:\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x} = \\frac{f(x+h,y) - f(x,y)}{h}$$\n",
    "\n",
    "Onde $h$ é pequeno. Nós vamos, então, calcular a saída inicial $f(x,y)$ e aumentar $x$ por um valor pequeno $h$ e calcular a nova saída $f(x+h,y)$. Então, nós subtraimos esse valores para ver a diferença e dividimos por $f(x+h,y)$ para normalizar essa mudança pelo valor (arbitrário) que nós usamos.\n",
    "\n",
    "Em termos de código, teremos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:26.300444Z",
     "start_time": "2017-09-12T19:39:26.165041Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = -2,3\n",
    "out = forwardMultiplyGate(x,y)\n",
    "h = 0.0001\n",
    "\n",
    "# derivada em relação a x\n",
    "\n",
    "\n",
    "# derivada em relação a y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a gente pode ver, a derivada em relação a $x$ é igual a $+3$. O sinal positivo indica que o alterando o valor de $x$ pelo passo $h$, a saída se torna maior. O valor $3$ pode ser considerado como o valor da força que puxa $x$. O inverso acontece com $y$.\n",
    "\n",
    "> *A derivada em relação a alguma entrada pode ser calculada ajustando levemente aquela entrada e observando a mudança no valor da saída*\n",
    "\n",
    "A derivada é calculada sobre cada entrada, enquanto o **gradiente** representa todas as derivadas sobre as entradas concatenadas em um vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:26.440557Z",
     "start_time": "2017-09-12T19:39:26.304947Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passo = 0.01\n",
    "out = forwardMultiplyGate(x,y)\n",
    "x = x + passo * derivada_x\n",
    "y = y + passo * derivada_y\n",
    "out_new = forwardMultiplyGate(x,y)\n",
    "\n",
    "print(out_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a gente pode perceber $-5.87 > -6$. Apenas 3 avaliações foram necessárias para aumentar o valor da saída (ao invés de centenas) e conseguimos um melhor resultado.\n",
    "\n",
    "**Passo maior nem sempre é melhor**: É importante destacar qualque valor de passo maior que 0.01 ia sempre funcionar melhor (por exemplo, passo = 1 gera a saída = 1). No entanto, a medida que os circuitos vão ficando mais complexos (como em redes neurais completas), a função vai ser tornando mais caótica e complexa. O gradiente garante que se você tem um passo muito pequeno (o ideal seria infinitesimal), então vou definitivamente aumenta a saída seguindo aquela direção. O passo que estamos utilizando (0.01) ainda é muito grande, mas como nosso circuito é simples, podemos esperar pelo melhor resultado. Lembre-se da analogia do **escalador cego**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estratégia 4: Gradiente Analítico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estratégia que utilizamos até agora de ajustar levemente a entrada e ver o que acontece pode não ser muito cômoda na prática quando temos milhares de entradas para ajustar. Então, a gente precisa de algo melhor.\n",
    "\n",
    "Felizmente, existe uma estratégia mais fácil e muito mais rápida para calcular o gradiente: podemos usar cálculo para derivar diretamente a nossa função. Chamamos isso de **gradiente analítico** e dessa forma não precisamos ajustar levemente nada. \n",
    "\n",
    "> *O gradiente analítico evita o leve ajustamento das entradas. O circuito pode ser derivado usando cálculo.*\n",
    "\n",
    "É muito fácil calcular derivadas parciais para funções simples como x$*$y. Se você não lembra da definição, aqui está o cálculo da derivada parcial em relação a x da nossa função $f(x,y)$:\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x} = \\frac{f(x+h,y) - f(x,y)}{h}\n",
    "= \\frac{(x+h)y - xy}{h}\n",
    "= \\frac{xy + hy - xy}{h}\n",
    "= \\frac{hy}{h}\n",
    "= y$$\n",
    "\n",
    "A derivada parcial em relação em $x$ da nossa $f(x,y)$ é igual $y$. Você reparou na coincidência de $\\partial x = 3.0$, que é exatamente o valor de $y$? E que o mesmo aconteceu para $x$? **Então, a gente não precisa ajustar nada!** E nosso código fica assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:26.571650Z",
     "start_time": "2017-09-12T19:39:26.444559Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = -2,3\n",
    "out = forwardMultiplyGate(x,y)\n",
    "\n",
    "# insira seu código aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante destacar que a Estratégia #3 reduziu a #2 para uma única vez. Porém, a #3 nos dá somente uma aproximação do gradiente, enquanto a Estratégia #4 nos dá o valor *exato*. Sem aproximações. O único lado negativo é que temos de saber derivar a nossa funcão.\n",
    "\n",
    "Recapitulando o que vimos até aqui:\n",
    "- __Estratégia 1__: definimos valores aleatórios em todas as iterações. Não funciona para muitas entradas.\n",
    "- __Estratégia 2__: pequenos ajustes aleatórios as entradas e vemos qual funciona melhor. Tão ruim quando a #1.\n",
    "- __Estratégia 3__: muito melhor através do cálculo do gradiente. Independentemente de quão complicado é o circuito, o **gradiente numérico** é muito simples de se calcular (mas um pouco caro).\n",
    "- __Estratégia 4__: no final, vimos que a forma melhor, mais inteligente e mais rápida é calcular o **gradiente analítico**. O resultado é idêntico ao gradiente numérico, porém mais rápido e não precisa de ajustes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso Recursivo: Múltiplas Portas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular o gradiente para o nosso circuito foi trivial. Mas, e em circuitos mais complexos? Como a gente vai ver agora, cada porta pode ser tratada individualmente e a gente pode computar derivadas locais como a gente fez anteriormente. Vamos considerar nossa função agora como segue:\n",
    "\n",
    "$$f(x,y,z) = (x+y)*z$$\n",
    "\n",
    "<img src='images/circuito_2.png' width='300'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:26.747159Z",
     "start_time": "2017-09-12T19:39:26.575653Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardAddGate(a, b):\n",
    "    return a+b\n",
    "\n",
    "def forwardCircuit(x,y,z):\n",
    "    q = forwardAddGate(x,y)\n",
    "    f = forwardMultiplyGate(q, z)\n",
    "    return f\n",
    "\n",
    "print(forwardCircuit(-2, 5, -4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos calcular agora a nossa derivada? Primeiramente, vamos esquecer da porta de soma e fingir que temos apenas duas entradas no nosso circuito: **q** e **z**. Como já vimos, as nossas derivadas parciais podem ser definidas como segue:\n",
    "\n",
    "$$f(q,z) = q z \\hspace{0.5in} \\implies \\hspace{0.5in} \\frac{\\partial f(q,z)}{\\partial q} = z, \\hspace{1in} \\frac{\\partial f(q,z)}{\\partial z} = q$$\n",
    "\n",
    "Ok, mas e em relação a x e y? Como q é calculado em função de x e y (pela adição em nosso exemplo), nós também podemos calcular suas derivadas parciais:\n",
    "\n",
    "$$q(x,y) = x + y \\hspace{0.5in} \\implies \\hspace{0.5in} \\frac{\\partial q(x,y)}{\\partial x} = 1, \\hspace{1in} \\frac{\\partial q(x,y)}{\\partial y} = 1$$\n",
    "\n",
    "Correto! As derivadas parciais são 1, independentemente dos valores de x e y. Isso faz sentido se pensarmos que para aumentar o saída de uma porta de adição, a gente espera uma força positiva tanto em x quanto em y, independente dos seus valores.\n",
    "\n",
    "Com as fórmulas acima, nós sabemos calcular o gradiente da saída em relação a q e z, e o gradiente de q em relação a x e y. Para calcular o gradiente do nosso circuito em relação a x, y e z, nós vamos utilizar a **Regra da Cadeia**, que vai nos dizer como combinar esses gradientes. A derivada final em relação a x, será dada por:\n",
    "\n",
    "$$\\frac{\\partial f(q,z)}{\\partial x} = \\frac{\\partial q(x,y)}{\\partial x} \\frac{\\partial f(q,z)}{\\partial q}$$\n",
    "\n",
    "Pode parecer complicado a uma primeira vista, mas a verdade é que isso vai ser simplificado a somente duas multiplicações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:26.889072Z",
     "start_time": "2017-09-12T19:39:26.753160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, z = -2, 5, -4\n",
    "q = forwardAddGate(x,y)\n",
    "f = forwardMultiplyGate(q,z)\n",
    "\n",
    "# Derivada da porta de multiplicação\n",
    "\n",
    "\n",
    "# Derivada da porta de adição\n",
    "\n",
    "\n",
    "# Regra da cadeia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/circuito_2_back.png\">\n",
    "\n",
    "É isso! Vamos agora fazer nossas entradas responder ao gradiente. Lembrando que queremos um valor maior que -12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:27.023174Z",
     "start_time": "2017-09-12T19:39:26.893080Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_f_rel_xyz = [der_f_rel_x, der_f_rel_y, der_f_rel_z]\n",
    "\n",
    "passo = 0.01\n",
    "x = x + passo * der_f_rel_x\n",
    "y = y + passo * der_f_rel_y\n",
    "z = z + passo * der_f_rel_z\n",
    "\n",
    "print(forwardCircuit(x,y,z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora analisar os resultados separadamente. Analisando primeiramente q e z, vemos que o circuito quer que z aumente (der_f_rel_z = +3) e o valor de q diminua (der_f_rel_q = -4) com uma força maior (4 contra 3).\n",
    "\n",
    "Em relação a porta de soma, como vimos, o padrão é que aumentando as entradas a saída também aumenta. Porém, o circuito quer que q diminua (der_f_rel_q = -4). Esse é o **ponto crucial**: em vez de aplicarmos uma força de +1 as entradas da porta de soma como normalmente faríamos (derivada local), o circuito quer que os gradientes em x e y se tornem 1x-4=-4. Isso faz sentido: o circuito quer x e y pequeno para que q seja pequeno também, o que vai aumentar f.\n",
    "\n",
    "> *Se isso fez sentido, você entendeu backpropagation.*\n",
    "\n",
    "**Recapitulando:**\n",
    "- Vimos que para uma simples porta (or simples expressão), podemos derivar o gradiente analítico usando cálculo simples. Nós interpretamos gradiente como uma força que puxa as entradas na direção necessária para fazer a saída aumentar.\n",
    "- No caso de múltiplas portas, cada porta é tratada individualmente até que o circuito é tratado como um todo. A *única* diferença é que agora o circuito diz como a saída de outras portas devem se comportar (como da porta de adição), que é o gradiente final do circuito em relação a saída da porta. É como o circuito pedindo aquela porta maior ou menor valor de saída, e com alguma força. A porta simplesmente pega essa força e multiplica em relação a todas as forças calculadas para suas entradas anteriores (regra da cadeia [repare como a força de q (-4) é multiplicada as forças de x e y]). Isso pode ter dois efeitos desejados:\n",
    "    - Se a porta tem uma força positiva de saída, essa força também é multiplicada nas suas entradas, escalonada pelo valor da força das entradas.\n",
    "    - Se a porta tem uma força negativa de saída, isso significa que o circuito quer que a saída decresça, então essa força é multiplicada pelas entradas para diminuir o valor de saída.\n",
    "\n",
    "> *Tenha em mente que a força da saída do circuito vai puxando as outras forças na direção desejada por todo o circuito até as entradas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checagem do gradiente numérico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar se os gradientes analíticos que calculamos por backpropagation estão corretos. Lembre-se que podemos fazer isso através do gradiente numérico e espero que o resultado seja [-4, -4, 4] para x,y,z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:27.209310Z",
     "start_time": "2017-09-12T19:39:27.027170Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y,z = -2,5,-4\n",
    "\n",
    "h = 0.0001\n",
    "\n",
    "#insira seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neurônio Sigmóide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualquer função diferenciável pode atuar como uma porta, como também podemos agrupar múltiplas portas para formar uma simples porta, ou decompor um função em múltiplas portas quando for conveniente. Para exemplificar, vamos utilizar a função de ativação *sigmoid* com entradas **x** e pesos **w**:\n",
    "\n",
    "$$f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$\n",
    "\n",
    "Como dito a função acima nada mais é que a função sigmoid $\\sigma(x)$. Sabendo, então, que a derivada da função sigmoid é:\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}=(1-\\sigma(x))\\sigma(x)$$\n",
    "\n",
    "Vamos calcular a gradiente em relação as entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:27.342454Z",
     "start_time": "2017-09-12T19:39:27.217305Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0, w1, w2 = 2, -3, -3\n",
    "x0, x1 = -1, -2\n",
    "\n",
    "# forward pass\n",
    "\n",
    "\n",
    "# backward pass\n",
    "\n",
    "\n",
    "# Nova saida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos supor agora que não sabemos a derivada da função $\\sigma(x)$ muito menos de $f(w,x)$. O que podemos fazer?.\n",
    "\n",
    "**Decompor essa função em circuito com múltiplas portas!** Dessa forma:\n",
    "\n",
    "<img src='images/circuito_3.png' width='800'>\n",
    "\n",
    "Calculando a saída para cada porta, temos:\n",
    "\n",
    "<img src='images/circuito_3_forward.png' width='800'>\n",
    "\n",
    "Onde sabemos as seguintes derivadas:\n",
    "\n",
    "$$f(x) = \\frac{1}{x} \\rightarrow \\frac{df}{dx} = -1/x^2 \n",
    "\\\\\\\\\n",
    "f_c(x) = c + x \\rightarrow \\frac{df}{dx} = 1 \n",
    "\\\\\\\\\n",
    "f(x) = e^x \\rightarrow \\frac{df}{dx} = e^x\n",
    "\\\\\\\\\n",
    "f_a(x) = ax \\rightarrow \\frac{df}{dx} = a$$\n",
    "\n",
    "Onde as funções $f_c(x)$ e $f_a(x)$ transladam a entrada por uma constante **c** e escala por uma contante **a**, respectivamente. Na verdade, são apenas casos especias de adição e multiplicação, mas que foram introduzidos como portas unárias.\n",
    "\n",
    "Como podemos calcular a derivada em relação as entradas agora? **Usando Backpropagation!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se tornando um Ninja em Backpropagation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de resolver o circuito acima, vamos praticar um pouco de backpropagation com alguns exemplos. Vamos esquecer funções por enquanto e trabalhar só com 4 variáveis: a, b, c, e x. E nos referir as seus gradientes como da, db, dc, e dx. Além disso, vamos assumir que dx é dado (ou é +1 como nos casos acima). Nosso primeiro exemplo é a porta $*$, que já conhecemos:\n",
    "\n",
    "$$x  = a * b$$\n",
    "\n",
    "$$da = b * dx$$\n",
    "$$db = a * dx$$\n",
    "\n",
    "Se você reparar bem, vai perceber que a porta $*$ atua como um *switcher* durante a backpropagation, ou seja, o gradiente de cada entrada é o valor da outra multiplicado pelo gradiente da anterior (regra da cadeia). Por outro lado, vamos analisar a porta +:\n",
    "\n",
    "$$x  = a + b$$\n",
    "\n",
    "$$da = 1.0 * dx$$\n",
    "$$db = 1.0 * dx$$\n",
    "\n",
    "Nesse caso, 1.0 é o gradiente local e a multiplicação é a nossa regra da cadeia. **E se fosse a adição de 3 números?**:\n",
    "\n",
    "$$q = a + b$$\n",
    "$$x = q + c$$\n",
    "\n",
    "$$dc = 1.0 * dx$$\n",
    "$$dq = 1.0 * dx$$\n",
    "$$da = 1.0 * dq$$\n",
    "$$db = 1.0 * dq$$\n",
    "\n",
    "Você percebe o que está acontecendo? Se você olhar nos diagramas dos circuitos que já resolvemos, vai perceber que a porta + simplesmente pega o gradiente atual e roteia igualmente para todas as entradas (porque os gradientes locais são sempre 1.0 para todas as entradas, independente dos seus valores atuais). Então, podemos fazer bem mais rápido:\n",
    "\n",
    "$$x = a + b + c$$\n",
    "\n",
    "$$da = 1.0 * dx$$\n",
    "$$db = 1.0 * dx$$\n",
    "$$dc = 1.0 * dx$$\n",
    "\n",
    "Okay. Mas, e se combinarmos portas?\n",
    "\n",
    "$$x = a*b + c$$\n",
    "\n",
    "$$da = b * dx$$\n",
    "$$db = a * dx$$\n",
    "$$dc = 1.0 * dx$$\n",
    "\n",
    "Se você não percebeu o que aconteceu, introduza uma variável temporária $q = a * b$ e então calcula $x = q + c$ para se convencer. E quanto a este exemplo:\n",
    "\n",
    "$$x = a * a$$\n",
    "$$da = 2 * a * dx$$\n",
    "\n",
    "Outro exemplo:\n",
    "\n",
    "$$x = a*a + b*b + c*c$$\n",
    "$$da = 2 * a * dx$$\n",
    "$$db = 2 * b * dx$$\n",
    "$$dc = 2 * c * dx$$\n",
    "\n",
    "Okay. Agora mais complexo:\n",
    "\n",
    "$$x = (a * b + c) * d)^2$$\n",
    "\n",
    "Quando casos mais complexos como esse acontecem, eu gosto de dividir a expressão em partes gerenciáveis que são quase sempre compostas de simples expressões onde eu posso aplicar a regra da cadeia:\n",
    "\n",
    "$$x1 = a * b + c$$\n",
    "$$x2 = x1 * d$$\n",
    "$$x = x2 * x2$$\n",
    "\n",
    "$$dx2 = 2 * x2 * dx$$\n",
    "$$dx1 = d * dx2$$\n",
    "$$dd = x1 * dx2$$\n",
    "$$da = b * dx1$$\n",
    "$$db = a * dx1$$\n",
    "$$dc = 1 * dx1$$\n",
    "\n",
    "Não foi tão difícil! Essas são as equações para toda a expressão, e nós fizemos dividindo peça por peça e aplicando backpropagação a todas as variáveis. Note que **toda variável durante a fase forward tem uma variável equivalente na backpropagação que contém o gradiente em relação a saída do circuito.**. Mais um exemplo útil de função e seu gradiente local:\n",
    "\n",
    "$$x = 1.0/a$$\n",
    "$$da = 1.0/(a*a) * dx$$\n",
    "\n",
    "E como ela pode ser aplicada na prática:\n",
    "\n",
    "$$x = (a+b)/(c+d)$$\n",
    "\n",
    "$$x1 = a + b$$\n",
    "$$x2 = c + d$$\n",
    "$$x3 = 1.0 / x2$$\n",
    "$$x  = x1 * x3$$\n",
    "\n",
    "$$dx1 = x3 * dx$$\n",
    "$$dx3 = x1 * dx$$\n",
    "$$dx2 = (1.0/(x2 * x2)) * dx3$$\n",
    "$$dc = 1 * dx2$$\n",
    "$$dd = 1 * dx2$$\n",
    "$$da = 1 * dx1$$\n",
    "$$db = 1 * dx1$$\n",
    "\n",
    "E mais uma:\n",
    "\n",
    "$$x = math.max(a, b)$$\n",
    "$$da = x == a\\ ?\\ 1.0 * dx\\ :\\ 0.0$$\n",
    "$$db = x == b\\ ?\\ 1.0 * dx\\ :\\ 0.0$$\n",
    "\n",
    "No caso acima é mais difícil de entender. A função **max** passa o valor para a maior entrada e ignora as outras. Na fase de backpropagation, a porta max simplesmente pega o gradiente atual e roteia para a entrada que teve o maior valor duranta a fase de forward. A porta age como um simples switch baseado na entrada com o maior valor durante a forward. As outras entradas terão gradiente zero.\n",
    "\n",
    "Agora, vamos dar uma olhada na porta **ReLU (*Rectified Linear Unit)***, muita usada em redes neurais no lugar da função sigmoid. Ela é simplesmente um threshold com zero:\n",
    "\n",
    "$$x = max(a, 0)$$\n",
    "$$da = a > 0\\ ?\\ 1.0 * dx\\ :\\ 0.0$$\n",
    "\n",
    "Em outras palavras, essa porta simplesmente passa o valor adiante se ele é maior que zero, ou interrompe o fluxo e seta o valor para zero. Na backpropagação, a porta vai passar o gradiente atual se ele foi ativado durante a forward, ou se a entrada original foi menor que zero, ela vai interromper o fluxo de gradiente.\n",
    "\n",
    "Finalmente, vamos ver como calcular o gradiente em operações vetorizadas que vamos utilizar muito em redes neurais:\n",
    "\n",
    "$$W = np.random.randn(5,10)$$\n",
    "$$X = np.random.randn(3,10)$$\n",
    "$$Y = X.dot(W^T)$$\n",
    "\n",
    "Supondo que o gradiente de Y é dado como a seguir:\n",
    "$$dY = np.random.randn(*Y.shape)$$\n",
    "$$dW = dY^T.dot(X)$$\n",
    "$$dX = dY.dot(W)$$\n",
    "\n",
    "Espero que tenha entendido como calcular expressões inteiras (que são feitas de muitas portas) e como calcular a backpropagação para cada uma delas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo dos Padrões na Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resumir os padrões no fluxo da backpropagation considere esse circuito:\n",
    "\n",
    "<img src='images/backpropagation_padroes.png' width='450'>\n",
    "\n",
    "A **porta de soma** simplesmente pega o gradiente na saída e distribui igualmente para entrada, independente dos valores durante a etapa de forward. Isso vem do fato que o gradiente local para a operação de adicionar é simplesmente +1.0, então os gradientes em todas as entradas vão ser exatamente iguais ao gradiente da saída porque ele vai ser multiplicado por x1.0 (e continua o mesmo). No circuito acima, repare como a porta + roteou o gradiente 2.0 para ambas as entradas, igualmente e sem alteração.\n",
    "\n",
    "A **porta max** roteia o gradiente. Diferente da porta de soma que distribui o gradiente para todas as entradas, distribui o gradiente (sem alteração) para exatamente uma das entradas (a que tinha o maior valor durante a etapa de forward). Isso acontece por que o gradiente local é 1.0 para o maior valor e 0.0 para os outros valores. No circuito acima, a operação max roteou o gradiente de 2.0 para a variável **z**, que tinha um valor maior que **w**, e o gradiente de w continua zero.\n",
    "\n",
    "A **porta de multiplicação** é um pouquinho mais difícil de interpretar. Os gradientes locais são os valores das entradas (cambiados) e multiplicados pelo gradiente da saída durante a regra da cadeia. No exemplo acima, o gradiente em **x** é -8.00, pois é igual a -4.00 x 2.00.\n",
    "\n",
    "*Efeitos não inutuitivos e suas consequências*. Note que se uma das entradas na porta de multiplicação é muito pequena e a outra é muito grande, então a porta de multiplicação vai fazer algo intuitivo: ela vai atribuir um gradiente muito alto para a menor entrada e um muito pequeno para a maior entrada. Perceba que no caso de classificadores lineares, onde os pesos são multiplicados com as entradas $w^Tx_i$, isso implica que os a escala dos dados tem um efeito na magnitude do gradiente para os pesos. Por exemplo, se você multiplicar todos os dados de entrada **$x_i$** por 1000 durante pré-processamento, então o gradiente dos pesos vão ser 1000x maior, e você terá de usar baixas taxas de aprendizagem para compensar o fator. Por isso que o pré-processamento é tão importante e o conhecimento intuitivo sobre os gradientes podem ajudar a debugar alguns desses casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementando o nosso neurônio\n",
    "\n",
    "<img src='images/circuito_3_back.png' width='800'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:27.492474Z",
     "start_time": "2017-09-12T19:39:27.347458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0, w1, w2 = 2, -3, -3\n",
    "x0, x1 = -1, -2\n",
    "\n",
    "# forward pass\n",
    "\n",
    "\n",
    "# backward pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver outro exemplo. Suponha que temos a seguinte função:\n",
    "\n",
    "$$f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}$$\n",
    "\n",
    "Só para deixar claro, essa função é completamente inútil, mas um bom exemplo de backpropagation na prática. Também é importante destacar que ela é bem difícil de derivar em relação a x  y. No entanto, como vimos, saber derivar uma função é completamente desnecessário por que não precisamos saber derivar a função inteira para calcular os gradientes. Só precisamos saber como calcular os gradientes locais. Aqui está a resolução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:39:27.640572Z",
     "start_time": "2017-09-12T19:39:27.500477Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = 3, -4\n",
    "\n",
    "# forward pass\n",
    "\n",
    "\n",
    "\n",
    "# backward pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare em algumas coisas importantes:\n",
    "\n",
    "**Variáveis temporárias para armazenar resultados**. Para calcular a backpropagation, é importante ter algumas (se não todas) das variáveis calculadas na etapa de forward. Na prática, é bom estruturar seu código de maneira a guardar esses valores para a backprop. Em último caso, você pode recalculá-las.\n",
    "\n",
    "**Gradientes adicionados**. A etapa de forward envolveu as variáveis x e y muitas vezes, então quando fizemos a backprop temos de ter cuidados de acumular o gradiente nessas variáveis (+=). Isso segue a **regra da cadeia multivariável** em cálculo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [CS231n - Optimization: Stochastic Gradient Descent](http://cs231n.github.io/optimization-1/) \n",
    "2. [CS231n - Backpropagation, Intuitions](http://cs231n.github.io/optimization-2/)\n",
    "3. [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
