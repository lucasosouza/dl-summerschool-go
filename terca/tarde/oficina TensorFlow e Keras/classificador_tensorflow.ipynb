{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exemplo Classificador de Flores - TensorFlow\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iris.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasosouza/anaconda/envs/udacity/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "COLUMN_NAMES = [\n",
    "        'SepalLength', \n",
    "        'SepalWidth',\n",
    "        'PetalLength', \n",
    "        'PetalWidth', \n",
    "        'Species'\n",
    "        ]\n",
    "\n",
    "# Importando dataset de treinamento\n",
    "training_dataset = pd.read_csv('iris_training.csv', names=COLUMN_NAMES, header=0)\n",
    "train_x = training_dataset.iloc[:, :4]\n",
    "train_y = training_dataset.iloc[:, 4]\n",
    "\n",
    "# Importando dataset de teste\n",
    "test_dataset = pd.read_csv('iris_test.csv', names=COLUMN_NAMES, header=0)\n",
    "test_x = test_dataset.iloc[:, :4]\n",
    "test_y = test_dataset.iloc[:, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iris_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Agora, precisamos definir colunas de recursos, que irão ajudar a nossa Rede Neural.\n",
    "\n",
    "columns_feat = [\n",
    "    tf.feature_column.numeric_column(key='SepalLength'),\n",
    "    tf.feature_column.numeric_column(key='SepalWidth'),\n",
    "    tf.feature_column.numeric_column(key='PetalLength'),\n",
    "    tf.feature_column.numeric_column(key='PetalWidth')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iris_modelo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/dl/zfkdh1395z33j_svnym2xm_00000gn/T/tmp4nad7zpv\n",
      "INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1311266a0>, '_tf_random_seed': None, '_task_id': 0, '_num_worker_replicas': 1, '_save_summary_steps': 100, '_session_config': None, '_service': None, '_task_type': 'worker', '_is_chief': True, '_save_checkpoints_secs': 600, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/dl/zfkdh1395z33j_svnym2xm_00000gn/T/tmp4nad7zpv', '_num_ps_replicas': 0, '_master': '', '_log_step_count_steps': 100, '_save_checkpoints_steps': None}\n"
     ]
    }
   ],
   "source": [
    "#Construindo a Rede Neural - DNNClassifier\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "\n",
    "    feature_columns=columns_feat,\n",
    "\n",
    "    # 2 camadas ocultas com 10 nós cada\n",
    "    hidden_units=[10, 10],\n",
    "\n",
    "    # O modelo está classificando 3 classes\n",
    "    n_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dataset in module tensorflow.python.data.ops.dataset_ops:\n",
      "\n",
      "class Dataset(builtins.object)\n",
      " |  Represents a potentially large set of elements.\n",
      " |  \n",
      " |  A `Dataset` can be used to represent an input pipeline as a\n",
      " |  collection of elements (nested structures of tensors) and a \"logical\n",
      " |  plan\" of transformations that act on those elements.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply(self, transformation_func)\n",
      " |      Apply a transformation function to this dataset.\n",
      " |      \n",
      " |      `apply` enables chaining of custom `Dataset` transformations, which are\n",
      " |      represented as functions that take one `Dataset` argument and return a\n",
      " |      transformed `Dataset`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```\n",
      " |      dataset = (dataset.map(lambda x: x ** 2)\n",
      " |                 .apply(group_by_window(key_func, reduce_func, window_size))\n",
      " |                 .map(lambda x: x ** 3))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        transformation_func: A function that takes one `Dataset` argument and\n",
      " |          returns a `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Dataset` returned by applying `transformation_func` to this dataset.\n",
      " |  \n",
      " |  batch(self, batch_size)\n",
      " |      Combines consecutive elements of this dataset into batches.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  cache(self, filename='')\n",
      " |      Caches the elements in this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
      " |          directory on the filesystem to use for caching tensors in this Dataset.\n",
      " |          If a filename is not provided, the dataset will be cached in memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  concatenate(self, dataset)\n",
      " |      Creates a `Dataset` by concatenating given dataset with this dataset.\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3 }\n",
      " |      b = { 4, 5, 6, 7 }\n",
      " |      \n",
      " |      # Input dataset and dataset to be concatenated should have same\n",
      " |      # nested structures and output types.\n",
      " |      # c = { (8, 9), (10, 11), (12, 13) }\n",
      " |      # d = { 14.0, 15.0, 16.0 }\n",
      " |      # a.concatenate(c) and a.concatenate(d) would result in error.\n",
      " |      \n",
      " |      a.concatenate(b) == { 1, 2, 3, 4, 5, 6, 7 }\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        dataset: `Dataset` to be concatenated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  filter(self, predicate)\n",
      " |      Filters this dataset according to `predicate`.\n",
      " |      \n",
      " |      Args:\n",
      " |        predicate: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          scalar `tf.bool` tensor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  flat_map(self, map_func)\n",
      " |      Maps `map_func` across this dataset and flattens the result.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  interleave(self, map_func, cycle_length, block_length=1)\n",
      " |      Maps `map_func` across this dataset, and interleaves the results.\n",
      " |      \n",
      " |      For example, you can use `Dataset.interleave()` to process many input files\n",
      " |      concurrently:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Preprocess 4 files concurrently, and interleave blocks of 16 records from\n",
      " |      # each file.\n",
      " |      filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\", ...\"]\n",
      " |      dataset = (Dataset.from_tensor_slices(filenames)\n",
      " |                 .interleave(lambda x:\n",
      " |                     TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
      " |                     cycle_length=4, block_length=16))\n",
      " |      ```\n",
      " |      \n",
      " |      The `cycle_length` and `block_length` arguments control the order in which\n",
      " |      elements are produced. `cycle_length` controls the number of input elements\n",
      " |      that are processed concurrently. If you set `cycle_length` to 1, this\n",
      " |      transformation will handle one input element at a time, and will produce\n",
      " |      identical results = to @{tf.data.Dataset.flat_map}. In general,\n",
      " |      this transformation will apply `map_func` to `cycle_length` input elements,\n",
      " |      open iterators on the returned `Dataset` objects, and cycle through them\n",
      " |      producing `block_length` consecutive elements from each iterator, and\n",
      " |      consuming the next input element each time it reaches the end of an\n",
      " |      iterator.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3, 4, 5 }\n",
      " |      \n",
      " |      # NOTE: New lines indicate \"block\" boundaries.\n",
      " |      a.interleave(lambda x: Dataset.from_tensors(x).repeat(6),\n",
      " |                   cycle_length=2, block_length=4) == {\n",
      " |          1, 1, 1, 1,\n",
      " |          2, 2, 2, 2,\n",
      " |          1, 1,\n",
      " |          2, 2,\n",
      " |          3, 3, 3, 3,\n",
      " |          4, 4, 4, 4,\n",
      " |          3, 3,\n",
      " |          4, 4,\n",
      " |          5, 5, 5, 5,\n",
      " |          5, 5,\n",
      " |      }\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: The order of elements yielded by this transformation is\n",
      " |      deterministic, as long as `map_func` is a pure function. If\n",
      " |      `map_func` contains any stateful operations, the order in which\n",
      " |      that state is accessed is undefined.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          `Dataset`.\n",
      " |        cycle_length: The number of elements from this dataset that will be\n",
      " |          processed concurrently.\n",
      " |        block_length: The number of consecutive elements to produce from each\n",
      " |          input element before cycling to another input element.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  make_initializable_iterator(self, shared_name=None)\n",
      " |      Creates an `Iterator` for enumerating the elements of this dataset.\n",
      " |      \n",
      " |      Note: The returned iterator will be in an uninitialized state,\n",
      " |      and you must run the `iterator.initializer` operation before using it:\n",
      " |      \n",
      " |      ```python\n",
      " |      dataset = ...\n",
      " |      iterator = dataset.make_initializable_iterator()\n",
      " |      # ...\n",
      " |      sess.run(iterator.initializer)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        shared_name: (Optional.) If non-empty, the returned iterator will be\n",
      " |          shared under the given name across multiple sessions that share the\n",
      " |          same devices (e.g. when using a remote server).\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Iterator` over the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If eager execution is enabled.\n",
      " |  \n",
      " |  make_one_shot_iterator(self)\n",
      " |      Creates an `Iterator` for enumerating the elements of this dataset.\n",
      " |      \n",
      " |      Note: The returned iterator will be initialized automatically.\n",
      " |      A \"one-shot\" iterator does not currently support re-initialization.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Iterator` over the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If eager execution is enabled.\n",
      " |  \n",
      " |  map(self, map_func, num_parallel_calls=None)\n",
      " |      Maps `map_func` across this datset.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having\n",
      " |          shapes and types defined by `self.output_shapes` and\n",
      " |         `self.output_types`) to another nested structure of tensors.\n",
      " |        num_parallel_calls: (Optional.) A `tf.int32` scalar `tf.Tensor`,\n",
      " |          representing the number elements to process in parallel. If not\n",
      " |          specified, elements will be processed sequentially.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  padded_batch(self, batch_size, padded_shapes, padding_values=None)\n",
      " |      Combines consecutive elements of this dataset into padded batches.\n",
      " |      \n",
      " |      Like `Dataset.dense_to_sparse_batch()`, this method combines\n",
      " |      multiple consecutive elements of this dataset, which might have\n",
      " |      different shapes, into a single element. The tensors in the\n",
      " |      resulting element have an additional outer dimension, and are\n",
      " |      padded to the respective shape in `padded_shapes`.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        padded_shapes: A nested structure of `tf.TensorShape` or\n",
      " |          `tf.int64` vector tensor-like objects representing the shape\n",
      " |          to which the respective component of each input element should\n",
      " |          be padded prior to batching. Any unknown dimensions\n",
      " |          (e.g. `tf.Dimension(None)` in a `tf.TensorShape` or `-1` in a\n",
      " |          tensor-like object) will be padded to the maximum size of that\n",
      " |          dimension in each batch.\n",
      " |        padding_values: (Optional.) A nested structure of scalar-shaped\n",
      " |          `tf.Tensor`, representing the padding values to use for the\n",
      " |          respective components.  Defaults are `0` for numeric types and\n",
      " |          the empty string for string types.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  prefetch(self, buffer_size)\n",
      " |      Creates a `Dataset` that prefetches elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          maximum number elements that will be buffered when prefetching.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  repeat(self, count=None)\n",
      " |      Repeats this dataset `count` times.\n",
      " |      \n",
      " |      NOTE: If this dataset is a function of global state (e.g. a random number\n",
      " |      generator), then different repetitions may produce different elements.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of times the dataset should be repeated. The default behavior\n",
      " |          (if `count` is `None` or `-1`) is for the dataset be repeated\n",
      " |          indefinitely.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  shard(self, num_shards, index)\n",
      " |      Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      " |      \n",
      " |      This dataset operator is very useful when running distributed training, as\n",
      " |      it allows each worker to read a unique subset.\n",
      " |      \n",
      " |      When reading a single input file, you can skip elements as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.TFRecordDataset(FLAGS.input_file)\n",
      " |      d = d.shard(FLAGS.num_workers, FLAGS.worker_index)\n",
      " |      d = d.repeat(FLAGS.num_epochs)\n",
      " |      d = d.shuffle(FLAGS.shuffle_buffer_size)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Important caveats:\n",
      " |      \n",
      " |      - Be sure to shard before you use any randomizing operator (such as\n",
      " |        shuffle).\n",
      " |      - Generally it is best if the shard operator is used early in the dataset\n",
      " |        pipeline. For example, when reading from a set of TFRecord files, shard\n",
      " |        before converting the dataset to input samples. This avoids reading every\n",
      " |        file on every worker. The following is an example of an efficient\n",
      " |        sharding strategy within a complete pipeline:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = Dataset.list_files(FLAGS.pattern)\n",
      " |      d = d.shard(FLAGS.num_workers, FLAGS.worker_index)\n",
      " |      d = d.repeat(FLAGS.num_epochs)\n",
      " |      d = d.shuffle(FLAGS.shuffle_buffer_size)\n",
      " |      d = d.repeat()\n",
      " |      d = d.interleave(tf.data.TFRecordDataset,\n",
      " |                       cycle_length=FLAGS.num_readers, block_length=1)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          shards operating in parallel.\n",
      " |        index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `num_shards` or `index` are illegal values. Note: error\n",
      " |          checking is done on a best-effort basis, and aren't guaranteed to be\n",
      " |          caught upon dataset creation. (e.g. providing in a placeholder tensor\n",
      " |          bypasses the early checking, and will instead result in an error during\n",
      " |          a session.run call.)\n",
      " |  \n",
      " |  shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)\n",
      " |      Randomly shuffles the elements of this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of elements from this dataset from which the new\n",
      " |          dataset will sample.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          random seed that will be used to create the distribution. See\n",
      " |          @{tf.set_random_seed} for behavior.\n",
      " |        reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
      " |          that the dataset should be pseudorandomly reshuffled each time it is\n",
      " |          iterated over. (Defaults to `True`.)\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  skip(self, count)\n",
      " |      Creates a `Dataset` that skips `count` elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number\n",
      " |          of elements of this dataset that should be skipped to form the\n",
      " |          new dataset.  If `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain no elements.  If `count`\n",
      " |          is -1, skips the entire dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  take(self, count)\n",
      " |      Creates a `Dataset` with at most `count` elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be taken to form the new dataset.\n",
      " |          If `count` is -1, or if `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain all elements of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_generator(generator, output_types, output_shapes=None)\n",
      " |      Creates a `Dataset` whose elements are generated by `generator`.\n",
      " |      \n",
      " |      The `generator` argument must be a callable object that returns\n",
      " |      an object that support the `iter()` protocol (e.g. a generator function).\n",
      " |      The elements generated by `generator` must be compatible with the given\n",
      " |      `output_types` and (optional) `output_shapes` arguments.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      import itertools\n",
      " |      \n",
      " |      def gen():\n",
      " |        for i in itertools.count(1):\n",
      " |          yield (i, [1] * i)\n",
      " |      \n",
      " |      ds = Dataset.from_generator(\n",
      " |          gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))\n",
      " |      value = ds.make_one_shot_iterator().get_next()\n",
      " |      \n",
      " |      sess.run(value)  # (1, array([1]))\n",
      " |      sess.run(value)  # (2, array([1, 1]))\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: The current implementation of `Dataset.from_generator()` uses\n",
      " |      @{tf.py_func} and inherits the same constraints. In particular, it\n",
      " |      requires the `Dataset`- and `Iterator`-related operations to be placed\n",
      " |      on a device in the same process as the Python program that called\n",
      " |      `Dataset.from_generator()`. The body of `generator` will not be\n",
      " |      serialized in a `GraphDef`, and you should not use this method if you\n",
      " |      need to serialize your model and restore it in a different environment.\n",
      " |      \n",
      " |      NOTE: If `generator` depends on mutable global variables or other external\n",
      " |      state, be aware that the runtime may invoke `generator` multiple times\n",
      " |      (in order to support repeating the `Dataset`) and at any time\n",
      " |      between the call to `Dataset.from_generator()` and the production of the\n",
      " |      first element from the generator. Mutating global variables or external\n",
      " |      state can cause undefined behavior, and we recommend that you explicitly\n",
      " |      cache any external state in `generator` before calling\n",
      " |      `Dataset.from_generator()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        generator: A callable object that takes no arguments and returns an\n",
      " |          object that supports the `iter()` protocol.\n",
      " |        output_types: A nested structure of `tf.DType` objects corresponding to\n",
      " |          each component of an element yielded by `generator`.\n",
      " |        output_shapes: (Optional.) A nested structure of `tf.TensorShape`\n",
      " |          objects corresponding to each component of an element yielded by\n",
      " |          `generator`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  from_sparse_tensor_slices(sparse_tensor)\n",
      " |      Splits each rank-N `tf.SparseTensor` in this dataset row-wise. (deprecated)\n",
      " |      \n",
      " |      THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use `tf.data.Dataset.from_tensor_slices()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_tensor: A `tf.SparseTensor`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset` of rank-(N-1) sparse tensors.\n",
      " |  \n",
      " |  from_tensor_slices(tensors)\n",
      " |      Creates a `Dataset` whose elements are slices of the given tensors.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A nested structure of tensors, each having the same size in the\n",
      " |          0th dimension.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  from_tensors(tensors)\n",
      " |      Creates a `Dataset` with a single element, comprising the given tensors.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A nested structure of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  list_files(file_pattern)\n",
      " |      A dataset of all files matching a pattern.\n",
      " |      \n",
      " |      Example:\n",
      " |        If we had the following files on our filesystem:\n",
      " |          - /path/to/dir/a.txt\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |        If we pass \"/path/to/dir/*.py\" as the directory, the dataset would\n",
      " |        produce:\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |      Args:\n",
      " |        file_pattern: A string or scalar string `tf.Tensor`, representing\n",
      " |          the filename pattern that will be matched.\n",
      " |      \n",
      " |      Returns:\n",
      " |       A `Dataset` of strings corresponding to file names.\n",
      " |  \n",
      " |  range(*args)\n",
      " |      Creates a `Dataset` of a step-separated range of values.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      Dataset.range(5) == [0, 1, 2, 3, 4]\n",
      " |      Dataset.range(2, 5) == [2, 3, 4]\n",
      " |      Dataset.range(1, 5, 2) == [1, 3]\n",
      " |      Dataset.range(1, 5, -2) == []\n",
      " |      Dataset.range(5, 1) == []\n",
      " |      Dataset.range(5, 1, -2) == [5, 3]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: follow same semantics as python's xrange.\n",
      " |          len(args) == 1 -> start = 0, stop = args[0], step = 1\n",
      " |          len(args) == 2 -> start = args[0], stop = args[1], step = 1\n",
      " |          len(args) == 3 -> start = args[0], stop = args[1, stop = args[2]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `RangeDataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if len(args) == 0.\n",
      " |  \n",
      " |  zip(datasets)\n",
      " |      Creates a `Dataset` by zipping together the given datasets.\n",
      " |      \n",
      " |      This method has similar semantics to the built-in `zip()` function\n",
      " |      in Python, with the main difference being that the `datasets`\n",
      " |      argument can be an arbitrary nested structure of `Dataset` objects.\n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3 }\n",
      " |      b = { 4, 5, 6 }\n",
      " |      c = { (7, 8), (9, 10), (11, 12) }\n",
      " |      d = { 13, 14 }\n",
      " |      \n",
      " |      # The nested structure of the `datasets` argument determines the\n",
      " |      # structure of elements in the resulting dataset.\n",
      " |      Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }\n",
      " |      Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }\n",
      " |      \n",
      " |      # The `datasets` argument may contain an arbitrary number of\n",
      " |      # datasets.\n",
      " |      Dataset.zip((a, b, c)) == { (1, 4, (7, 8)),\n",
      " |                                  (2, 5, (9, 10)),\n",
      " |                                  (3, 6, (11, 12)) }\n",
      " |      \n",
      " |      # The number of elements in the resulting dataset is the same as\n",
      " |      # the size of the smallest dataset in `datasets`.\n",
      " |      Dataset.zip((a, d)) == { (1, 13), (2, 14) }\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        datasets: A nested structure of datasets.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  output_classes\n",
      " |      Returns the class of each component of an element of this dataset.\n",
      " |      \n",
      " |      The expected values are `tf.Tensor` and `tf.SparseTensor`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of Python `type` objects corresponding to each\n",
      " |        component of an element of this dataset.\n",
      " |  \n",
      " |  output_shapes\n",
      " |      Returns the shape of each component of an element of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of `tf.TensorShape` objects corresponding to each\n",
      " |        component of an element of this dataset.\n",
      " |  \n",
      " |  output_types\n",
      " |      Returns the type of each component of an element of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of `tf.DType` objects corresponding to each component\n",
      " |        of an element of this dataset.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_GeneratorState', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_as_variant_tensor', '_enumerate', '_output_classes', '_output_shapes', '_output_types', '_tensors', 'apply', 'batch', 'cache', 'concatenate', 'filter', 'flat_map', 'from_generator', 'from_sparse_tensor_slices', 'from_tensor_slices', 'from_tensors', 'interleave', 'list_files', 'make_initializable_iterator', 'make_one_shot_iterator', 'map', 'output_classes', 'output_shapes', 'output_types', 'padded_batch', 'prefetch', 'range', 'repeat', 'shard', 'shuffle', 'skip', 'take', 'zip']\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/dl/zfkdh1395z33j_svnym2xm_00000gn/T/tmp4nad7zpv/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 213.73167\n",
      "INFO:tensorflow:global_step/sec: 656.13\n",
      "INFO:tensorflow:step = 101, loss = 24.065096 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 806.223\n",
      "INFO:tensorflow:step = 201, loss = 10.804886 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 815.901\n",
      "INFO:tensorflow:step = 301, loss = 9.9724045 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 843.127\n",
      "INFO:tensorflow:step = 401, loss = 7.998588 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 888.336\n",
      "INFO:tensorflow:step = 501, loss = 7.3980594 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 880.599\n",
      "INFO:tensorflow:step = 601, loss = 4.865939 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 873.981\n",
      "INFO:tensorflow:step = 701, loss = 4.8238134 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 877.202\n",
      "INFO:tensorflow:step = 801, loss = 6.348065 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 891.487\n",
      "INFO:tensorflow:step = 901, loss = 6.419024 (0.112 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/dl/zfkdh1395z33j_svnym2xm_00000gn/T/tmp4nad7zpv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 9.5643.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x11a6885c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definindo função de treinamento\n",
    "\n",
    "def train_function(inputs, outputs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(inputs), outputs))\n",
    "    print(dir(dataset))\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    return dataset #.make_one_shot_iterator().get_next()\n",
    "\n",
    "#Treinar o Modelo\n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda:train_function(train_x, train_y, 100),\n",
    "    steps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-06-17:00:30\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/dl/zfkdh1395z33j_svnym2xm_00000gn/T/tmp4nad7zpv/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-06-17:00:30\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.96666664, average_loss = 0.056468733, global_step = 1000, loss = 1.694062\n"
     ]
    }
   ],
   "source": [
    "#Definindo função de avaliação\n",
    "def evaluation_function(attributes, classes, batch_size):\n",
    "    attributes=dict(attributes)\n",
    "    if classes is None:\n",
    "        inputs = attributes\n",
    "    else:\n",
    "        inputs = (attributes, classes)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset # .make_one_shot_iterator().get_next()\n",
    "\n",
    "# Avaliando o classificador\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:evaluation_function(test_x, test_y, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:udacity]",
   "language": "python",
   "name": "conda-env-udacity-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
